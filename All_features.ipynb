{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#from AI.Features.Segmenting import deep_segment\n",
    "#from AI.Features.SegmentingV2 import deep_segment\n",
    "from AI.Features.SegmentingV8 import deep_segment\n",
    "\n",
    "#from AI.Features.Segmenting import restore_mask\n",
    "from AI.Features.SegmentingV8 import restore_mask\n",
    "\n",
    "from skimage import measure\n",
    "import glob\n",
    "import pandas as pd \n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score,f1_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "from skimage import feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lymph = [cv2.imread(f)[:,:,::-1] for f in glob.glob('./samples/PBC/LYMPH/*.jpg')]\n",
    "neut =  [cv2.imread(f)[:,:,::-1] for f in glob.glob('./samples/PBC/NEUT/*.jpg')]\n",
    "BASO =  [cv2.imread(f)[:,:,::-1] for f in glob.glob('./samples/PBC/BASO/*.jpg')]\n",
    "EOSI =  [cv2.imread(f)[:,:,::-1] for f in glob.glob('./samples/PBC/EOSI/*.jpg')]\n",
    "MONO =  [cv2.imread(f)[:,:,::-1] for f in glob.glob('./samples/PBC/MONO/*.jpg')]\n",
    "\n",
    "ln=[len(MONO),len(lymph),len(neut),len(BASO),len(EOSI)]\n",
    "print(ln)\n",
    "ls5=len(MONO)*[0]+len(lymph)*[1] + len(neut)*[2]+len(BASO)*[3]+len(EOSI)*[4]\n",
    "p=plt.hist(ls5,10)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#path1='./samples/MHS_Labeled/Human-Healthy-WrightGiemsa-13_tiles_cropped/Human-Healthy-WrightGiemsa-13_tiles_cropped/'\n",
    "#path1='./samples/MHS_Labeled/Human-Healthy-HE-10-18-MGG46_tiles_cropped/Human-Healthy-Giemsa-20_tiles_cropped/Human-Healthy-Giemsa-20_tiles_cropped/'\n",
    "path1='./samples/MHS_Labeled\\Human-Healthy-HE-10-18-MGG46_tiles_cropped/Human-Healthy-HE-10_tiles_cropped/Human-Healthy-HE-10_tiles_cropped/'          \n",
    "#path1='./samples/MHS_Labeled\\Human-Healthy-HE-10-18-MGG46_tiles_cropped/Human-MGG-46_tiles_cropped/Human-MGG-46_tiles_cropped/'\n",
    "    \n",
    "lymph = [cv2.imread(f)[:,:,::-1] for f in glob.glob(path1+'Lymphocyte/*.png')]\n",
    "neut =  [cv2.imread(f)[:,:,::-1] for f in glob.glob(path1+'Neutrophil/*.png')]\n",
    "BASO =  [cv2.imread(f)[:,:,::-1] for f in glob.glob(path1+'Basophil/*.png')]\n",
    "EOSI =  [cv2.imread(f)[:,:,::-1] for f in glob.glob(path1+'Eosinophil/*.png')]\n",
    "MONO =  [cv2.imread(f)[:,:,::-1] for f in glob.glob(path1+'Monocyte/*.png')]\n",
    "\n",
    "ln=[len(lymph),len(neut),len(MONO),len(BASO),len(EOSI)]\n",
    "print('lymph=',len(lymph))\n",
    "print('neut=',len(neut))\n",
    "print('MONO=',len(MONO))\n",
    "print('BASO=',len(BASO))\n",
    "print('EOSI=',len(EOSI))\n",
    "\n",
    "print(ln)\n",
    "print(sum(ln))\n",
    "ls5=len(lymph)*[0] + len(neut)*[1]+len(MONO)*[2]+len(BASO)*[3]+len(EOSI)*[4]\n",
    "p=plt.hist(ls5,10)\n",
    "plt.show()\n",
    "\n",
    "y = np.array(ln)\n",
    "mylabels =[\"lymph\",\"neut\",\"MONO\",\"BASO\",\"EOSI\"] \n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.pie(y, labels = mylabels, startangle = 10)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " \n",
    "\n",
    " \n",
    "\n",
    "# BCCD Dataset The labels (1- 5) represent neutrophil, lymphocyte, monocyte, eosinophil and basophil, respectively.\n",
    "\n",
    "data1 = [cv2.imread(f)[:,:,::-1] for f in glob.glob('./samples/BCCD/Dataset1/*.bmp')]\n",
    "data2 = [cv2.imread(f)[:,:,::-1] for f in glob.glob('./samples/BCCD/Dataset2/*.bmp')]\n",
    "\n",
    "\n",
    "labels1 = pd.read_csv(\"./samples/BCCD/Class Labels of Dataset 1.csv\") \n",
    "lst1=labels1['class label'].tolist()\n",
    "\n",
    "labels2 = pd.read_csv(\"./samples/BCCD/Class Labels of Dataset 2.csv\") \n",
    "lst2=labels2['class'].tolist()\n",
    "\n",
    "ims=data1#data2+data1\n",
    "lst=lst1# lst2+lst1\n",
    "\n",
    "neut = [ims[i] for i,c in enumerate(lst) if c==1] #+neut\n",
    "lymph = [ims[i] for i,c in enumerate(lst) if c==2] #+lymph\n",
    "MONO =  [ims[i] for i,c in enumerate(lst) if c==3] #+MONO\n",
    "EOSI = [ims[i] for i,c in enumerate(lst) if c==4] #+EOSI \n",
    "BASO =  [ims[i] for i,c in enumerate(lst) if c==5] #+BASO\n",
    "print([lst1.count(1),lst1.count(2),lst1.count(3),lst1.count(4),lst1.count(5)])\n",
    "print([lst2.count(1),lst2.count(2),lst2.count(3),lst2.count(4),lst2.count(5)])\n",
    "hist = labels1.hist(column='class label')\n",
    "hist = labels2.hist(column='class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from skimage.feature import greycomatrix, greycoprops\n",
    "from skimage import io, color, img_as_ubyte\n",
    "def GLCM(c):\n",
    "    img =c# io.imread('ff.jpg')\n",
    "\n",
    "    gray = color.rgb2gray(img)\n",
    "    image = img_as_ubyte(gray)\n",
    "    #io.imshow(image)\n",
    "\n",
    "    bins = np.array([0, 16, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 255]) #16-bit\n",
    "    inds = np.digitize(image, bins)\n",
    "\n",
    "    max_value = inds.max()+1\n",
    "    matrix_coocurrence = greycomatrix(inds, [1], [0, np.pi/4, np.pi/2, 3*np.pi/4], levels=max_value, normed=False, symmetric=False)\n",
    "\n",
    "    # GLCM properties\n",
    "    def contrast_feature(matrix_coocurrence):\n",
    "        contrast = greycoprops(matrix_coocurrence, 'contrast')\n",
    "        return \"Contrast = \", contrast\n",
    "\n",
    "    def dissimilarity_feature(matrix_coocurrence):\n",
    "        dissimilarity = greycoprops(matrix_coocurrence, 'dissimilarity')    \n",
    "        return \"Dissimilarity = \", dissimilarity\n",
    "\n",
    "    def homogeneity_feature(matrix_coocurrence):\n",
    "        homogeneity = greycoprops(matrix_coocurrence, 'homogeneity')\n",
    "        return \"Homogeneity = \", homogeneity\n",
    "\n",
    "    def energy_feature(matrix_coocurrence):\n",
    "        energy = greycoprops(matrix_coocurrence, 'energy')\n",
    "        return \"Energy = \", energy\n",
    "\n",
    "    def correlation_feature(matrix_coocurrence):\n",
    "        correlation = greycoprops(matrix_coocurrence, 'correlation')\n",
    "        return \"Correlation = \", correlation\n",
    "   \n",
    "    def ASM_feature(matrix_coocurrence):\n",
    "        ASM = greycoprops(matrix_coocurrence, 'ASM')\n",
    "        return \"ASM = \", ASM\n",
    "\n",
    "    \n",
    "    f1=contrast_feature(matrix_coocurrence)[1][0].tolist()\n",
    "    f2=dissimilarity_feature(matrix_coocurrence)[1][0].tolist()\n",
    "    f3=homogeneity_feature(matrix_coocurrence)[1][0].tolist()\n",
    "    f4=energy_feature(matrix_coocurrence)[1][0].tolist()\n",
    "    f5=correlation_feature(matrix_coocurrence)[1][0].tolist()\n",
    "    f6=ASM_feature(matrix_coocurrence)[1][0].tolist()\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    print(contrast_feature(matrix_coocurrence))\n",
    "    print(dissimilarity_feature(matrix_coocurrence))\n",
    "    print(homogeneity_feature(matrix_coocurrence))\n",
    "    print(energy_feature(matrix_coocurrence))\n",
    "    print(correlation_feature(matrix_coocurrence))\n",
    "    print(ASM_feature(matrix_coocurrence))\n",
    "    ent=measure.shannon_entropy( cv2.cvtColor(lymph[30], cv2.COLOR_BGR2GRAY) )\n",
    "    print(ent)\n",
    "\n",
    "    from skimage.morphology import disk, ball\n",
    "    from skimage.filters import rank\n",
    "    im2=rank.entropy(gray,disk(3)) \n",
    "    plt.imshow(im2,cmap='gray')\n",
    "    plt.show()\n",
    "    \"\"\"\n",
    "    return  np.array(f1+f2+f3+f4+f5+f6)\n",
    "    \n",
    "GLCM(neut[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from skimage.transform import integral_image\n",
    "from skimage.feature import haar_like_feature\n",
    "img = cv2.cvtColor(lymph[4], cv2.COLOR_BGR2GRAY)  \n",
    "img_ii = integral_image(img)\n",
    "feature = haar_like_feature(img_ii, 0, 0, 5, 5, 'type-3-x')\n",
    "feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.feature import shape_index\n",
    "image = color.rgb2gray(lymph[4])\n",
    "#image = img_as_ubyte(image)\n",
    "square =image\n",
    "s = shape_index(square, sigma=0.1)\n",
    "np.floor(s*10)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nuc_and_cyt(segmented):\n",
    "    #nuc = (segmented==2).astype('uint8')\n",
    "    #cyt = (segmented==1).astype('uint8')\n",
    "    cyt,nuc=restore_mask(segmented)\n",
    "    return cyt,nuc\n",
    "     \n",
    "def mask_cell(img):\n",
    "    im_mask=deep_segment(img)\n",
    "    #print(im_mask.sum())\n",
    "    if im_mask.sum()<1000:\n",
    "        print(\"No Cell detected\")\n",
    "        return img\n",
    "    cyt,nuc= get_nuc_and_cyt(im_mask)\n",
    "    if cyt.shape[0]!=0:\n",
    "        #reg=measure.regionprops(cyt)\n",
    "        tb=measure.regionprops(cyt)[0]['bbox']\n",
    "    else:\n",
    "        print(\"No cell detected...........\")\n",
    "        tb=(0,0,img.shape[0]-1,img.shape[1]-1)\n",
    "    #img=cv2.blur(img,(3,3))                #++\n",
    "    #img=cv2.medianBlur(img,15)\n",
    "     \n",
    "    im=img.copy()\n",
    "    rows=im_mask.shape[0]\n",
    "    columns=im_mask.shape[1]\n",
    "    for i in range(rows):\n",
    "        for j in range(columns):\n",
    "            if im_mask[i,j]==0:\n",
    "                im[i,j,:]=0\n",
    "    im=im[tb[0]:tb[2],tb[1]:tb[3]]  #to crop it around box\n",
    "    #im=cv2.resize(im,(100,100))\n",
    "    return im\n",
    "\n",
    "img=mask_cell(neut[11])\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img3=neut[11]\n",
    "mas=deep_segment(img3)\n",
    "nuc2, cyt2=restore_mask(mas)\n",
    "plt.imshow(img3)\n",
    "plt.show()\n",
    "plt.imshow(nuc2)\n",
    "plt.show()\n",
    "plt.imshow(cyt2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from skimage.feature import hog\n",
    "from skimage import data, exposure\n",
    "\n",
    "def HOG_im(c):\n",
    "    image = mask_cell(c)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) \n",
    "\n",
    "    fd, hog_image = hog(image, orientations=8, pixels_per_cell=(16, 16),\n",
    "                        cells_per_block=(1, 1), visualize=True,feature_vector=True)\n",
    "\n",
    "    \n",
    "    return fd,hog_image\n",
    "\n",
    "\n",
    "image=neut[14]\n",
    "fd,hog_image=HOG_im(image)\n",
    "\n",
    "image=mask_cell(image)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4), sharex=True, sharey=True)\n",
    "ax1.axis('off')\n",
    "ax1.imshow(image, cmap=plt.cm.gray)\n",
    "ax1.set_title('Input image')\n",
    "\n",
    "# Rescale histogram for better display\n",
    "hog_image_rescaled = exposure.rescale_intensity(hog_image, in_range=(0, 10))\n",
    "ax2.axis('off')\n",
    "ax2.imshow(hog_image_rescaled, cmap=plt.cm.gray)\n",
    "ax2.set_title('Histogram of Oriented Gradients')\n",
    "plt.show()\n",
    "len(fd)\n",
    "fd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "pbc =  [cv2.imread(f)[:,:,::-1] for f in glob.glob('./samples/PBC/NEUT/*.jpg')]\n",
    "plt.imshow(pbc[10])\n",
    "plt.show()\n",
    "\n",
    "d1 = [data1[i] for i,c in enumerate(lst1) if c==1] #+neut\n",
    "plt.imshow(d1[10])\n",
    "plt.show()\n",
    "\n",
    "d2 = [data2[i] for i,c in enumerate(lst2) if c==1] #+neut\n",
    "plt.imshow(d2[20])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extent feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_extent(nuc, cyt):\n",
    "    \n",
    "    # We only use the nucleus\n",
    "    extent = measure.regionprops(nuc)[0]['extent']\n",
    "    \n",
    "    return extent\n",
    "\n",
    "def all_together(cell):\n",
    "    segmented = deep_segment(cell)\n",
    "    nuc, cyt = get_nuc_and_cyt(segmented)\n",
    "    extent = get_extent(nuc,cyt)\n",
    "    return extent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'eccentricity', 'solidity', 'extent', 'circularity', 'nuc_cyto_ratio', 'size', 'tortuosity'\n",
    "from skimage import morphology\n",
    "\n",
    "def eccentricity(im_part):\n",
    "    tb=measure.regionprops(im_part)[0]['bbox']\n",
    "    \n",
    "    xb=abs(tb[1]-tb[3])\n",
    "    xs=abs(tb[0]-tb[2])\n",
    "    if xs>xb:\n",
    "        tm=xb\n",
    "        xb=xs\n",
    "        xs=tm\n",
    "    return np.sqrt(xb**2-xs**2)/xs\n",
    "    \n",
    "def Solidity(im_part):\n",
    "    conv=morphology.convex_hull.convex_hull_image(im_part)  \n",
    "    \"\"\"\n",
    "    print(im_part.sum())\n",
    "    print(conv.sum())\n",
    "    plt.imshow(im_part)\n",
    "    plt.show()\n",
    "    plt.imshow(conv)\n",
    "    plt.show()\n",
    "    \"\"\" \n",
    "    return im_part.sum()/conv.sum()\n",
    "\n",
    "def circularity(im_part):\n",
    "    pr=measure.perimeter(im_part, neighbourhood=8)\n",
    "    return (pr**2)/(4*np.pi*im_part.sum())\n",
    "\n",
    "def Convexity(im_part):\n",
    "    conv=morphology.convex_hull.convex_hull_image(im_part)  \n",
    "    return measure.perimeter(conv)/measure.perimeter(im_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "img=neut[13]\n",
    "segmented = deep_segment(img)\n",
    "nuc, cyt = get_nuc_and_cyt(segmented)\n",
    "print(eccentricity(nuc),eccentricity(nuc+cyt))\n",
    "print(Solidity(nuc),Solidity(nuc+cyt))\n",
    "print(circularity(nuc),circularity(nuc+cyt))\n",
    "print(Convexity(nuc),Convexity(nuc+cyt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shape_features(image):\n",
    "    segmented = deep_segment(image)\n",
    "    nuc, cyt = get_nuc_and_cyt(segmented)\n",
    "    all_cyt=nuc+cyt\n",
    "    n_sum=nuc.sum()\n",
    "    cyt_sum=all_cyt.sum()\n",
    "    lst=[eccentricity(nuc),eccentricity(all_cyt),Solidity(nuc),Solidity(all_cyt),circularity(nuc),\n",
    "         circularity(all_cyt),Convexity(nuc),Convexity(all_cyt)]\n",
    "    lst.append(measure.perimeter(nuc, neighbourhood=8)/n_sum)\n",
    "    lst.append(measure.perimeter(cyt, neighbourhood=8)/cyt_sum)\n",
    "    lst.append(n_sum/cyt_sum)\n",
    "    return np.array(lst)\n",
    "    \n",
    "img=neut[13]    \n",
    "shape_features(img)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmented = deep_segment(lymph[23])\n",
    "nuc, cyt = get_nuc_and_cyt(segmented)\n",
    "eccentricity(cyt)\n",
    "cyt.sum()\n",
    "plt.imshow(nuc)\n",
    "contours, hierarchy = cv2.findContours(nuc,cv2.RETR_LIST,cv2.CHAIN_APPROX_NONE)\n",
    "hull = [cv2.convexHull(cnt) for cnt in contours]\n",
    "\n",
    "nn=np.zeros((nuc.shape[0],nuc.shape[0]),dtype=int)\n",
    "for z in contours[0]:\n",
    "    nn[z[0][0],z[0][1]]=1\n",
    "plt.imshow(nn)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmented = deep_segment(lymph[13])\n",
    "nuc, cyt = get_nuc_and_cyt(segmented)\n",
    "#print(all_together(MONO[23]))\n",
    "plt.imshow(cyt)\n",
    "plt.show()\n",
    "plt.imshow(nuc)\n",
    "plt.show()\n",
    "print(nuc[nuc==0].shape)\n",
    "print(nuc[nuc==1].shape)\n",
    "print(nuc[nuc==1].shape[0]+nuc[nuc==0].shape[0])\n",
    "nuc.shape[0]*nuc.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "segmented = deep_segment(lymph[23])\n",
    "nuc, cyt = get_nuc_and_cyt(segmented)\n",
    "print(all_together(MONO[23]))\n",
    "plt.imshow(nuc)\n",
    "plt.show()\n",
    "print(nuc.min())\n",
    "tb=measure.regionprops(nuc)[0]['bbox']\n",
    "print(tb)\n",
    "im2=nuc[tb[0]:tb[2],tb[1]:tb[3]]\n",
    "im2.shape\n",
    "plt.imshow(im2)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "r=nuc.shape[0]\n",
    "c=nuc.shape[1]\n",
    "\n",
    "from skimage import feature\n",
    "edges1 = feature.canny(nuc*255,sigma=2)\n",
    "print(edges1)\n",
    "#edges2=cv2.threshold(edges1, 127, 255, cv2.THRESH_BINARY)[1]\n",
    "plt.imshow(edges1)\n",
    "plt.show()\n",
    "e2=cv2.Laplacian(nuc*255, 2, ksize=3)\n",
    "plt.imshow(e2)\n",
    "edges1.sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LBP Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    " \n",
    "from skimage import feature\n",
    "import numpy as np\n",
    "class LocalBinaryPatterns:  \n",
    "    def __init__(self, numPoints, radius):\n",
    "        # store the number of points and radius\n",
    "        self.numPoints = numPoints\n",
    "        self.radius = radius\n",
    "    def describe(self, image, eps=1e-7):\n",
    "        \n",
    "        # compute the Local Binary Pattern representation\n",
    "        # of the image, and then use the LBP representation\n",
    "        # to build the histogram of patterns\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        lbp = feature.local_binary_pattern(image, self.numPoints,\n",
    "            self.radius, method=\"uniform\")\n",
    "        (hist, _) = np.histogram(lbp.ravel(),\n",
    "            bins=np.arange(0, self.numPoints + 3),\n",
    "            range=(0, self.numPoints + 2))\n",
    "        # normalize the histogram\n",
    "        hist = hist.astype(\"float\")\n",
    "        hist /= (hist.sum() + eps)\n",
    "        # return the histogram of Local Binary Patterns\n",
    "        return hist\n",
    " \n",
    "\n",
    "points=24  #24\n",
    "\n",
    "obj=LocalBinaryPatterns(points,8)\n",
    "\n",
    "v=obj.describe(mask_cell(neut[7]))\n",
    " \n",
    "v\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sum Of Edges (SOE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edges1(im):\n",
    "    for i in range(im.shape[0]-1):\n",
    "        for j in range(im.shape[1]-1):\n",
    "            if im[i,j]==0 and im[i,j+1]==1:\n",
    "                im[i,j]=1\n",
    "            elif im[i,j]==1 and im[i,j+1]==0:\n",
    "                im[i,j]=1\n",
    "            else:\n",
    "                im[i,j]=0\n",
    "    return im\n",
    "\n",
    "def count_list(ls):\n",
    "    ls2=[0,0,0,0,0]\n",
    "    for x in ls:\n",
    "        if x==1:\n",
    "            ls2[0]=ls2[0]+1\n",
    "        elif x==2:\n",
    "            ls2[1]=ls2[1]+1\n",
    "        elif x==3:\n",
    "            ls2[2]=ls2[2]+1\n",
    "        elif x==4:\n",
    "            ls2[3]=ls2[3]+1\n",
    "        elif x>4:\n",
    "            ls2[4]=ls2[4]+1\n",
    "    return ls2\n",
    "        \n",
    "def SOE(c):\n",
    "    segmented = deep_segment(c)  \n",
    "    nuc, cyt = get_nuc_and_cyt(segmented)\n",
    "    \n",
    "    tb=measure.regionprops(cyt)[0]['bbox']\n",
    "    \n",
    "    xv=abs(tb[1]-tb[3])\n",
    "    xh=abs(tb[0]-tb[2])\n",
    "    \n",
    "    im=nuc.copy()\n",
    "    im2=edges1(im)\n",
    "    im=nuc.copy()\n",
    "    im3=edges1(im.T)\n",
    "\n",
    "    v2=im2.sum(axis=1)\n",
    "    v3=im3.sum(axis=1)\n",
    "    im_ls=v2.tolist()+v3.tolist()\n",
    "    #cn=(xh+xv)/2\n",
    "    freq=np.array(count_list(im_ls))\n",
    "    return freq/100  #np.array([freq[3]/freq[1]]) #np.array([ freq[2]/freq[1],freq[3]/freq[1],freq[4]/freq[1] ])    #/100#cn #(len(v2)+len(v3))\n",
    "\n",
    "#### Examples##################\n",
    "segmented = deep_segment(lymph[11])  \n",
    "nuc, cyt = get_nuc_and_cyt(segmented)\n",
    "tb=measure.regionprops(cyt)[0]['bbox']\n",
    "\n",
    "imt=edges1(nuc.T)\n",
    "imt=imt[tb[0]:tb[2],tb[1]:tb[3]] \n",
    "print(imt.sum(axis=1))\n",
    "plt.imshow(imt)\n",
    "plt.show()\n",
    "\n",
    "SOE(neut[12])  #np.divide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LBP and Extent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj=LocalBinaryPatterns(points,8)   #(58,4)\n",
    "def LBP_and_extent(c):\n",
    "\n",
    "    ss=obj.describe(mask_cell(c))\n",
    "    ss=ss.tolist()\n",
    "    ss.append(all_together(c)/10)\n",
    "    ss=np.array(ss)\n",
    "    return ss\n",
    "LBP_and_extent(lymph[-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj=LocalBinaryPatterns(points,8)   #(58,4)\n",
    "def SOE_LBP_and_extent(c):\n",
    "    ss=obj.describe(mask_cell(c))\n",
    "    ss=ss.tolist()\n",
    "    s2=SOE(c)\n",
    "    s2=s2.tolist()\n",
    "    ss=ss+s2\n",
    "    ss.append(all_together(c)/10)\n",
    "    ss=np.array(ss)\n",
    "    return ss\n",
    "SOE_LBP_and_extent(neut[7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SOE+Extent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj=LocalBinaryPatterns(points,8)   #(58,4)\n",
    "def SOE_and_extent(c):\n",
    "\n",
    "    ss=SOE(c)\n",
    "    ss=ss.tolist()\n",
    "    ss.append(all_together(c)/10)\n",
    "    ss=np.array(ss)\n",
    "    return ss\n",
    "SOE_and_extent(lymph[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shape Features+ SOE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def shapef_and_SOE(c):\n",
    "\n",
    "    ss=SOE(c)\n",
    "    ss=ss.tolist()\n",
    "    s2=shape_features(c)\n",
    "    s2=s2.tolist()\n",
    "    ss=ss+s2\n",
    "    ss=np.array(ss)\n",
    "    return ss\n",
    "shapef_and_SOE(lymph[-4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shape Features+ SOE+Extent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shapef_and_SOE_extent(c):\n",
    "\n",
    "    ss=SOE(c)\n",
    "    ss=ss.tolist()\n",
    "    s2=shape_features(c)\n",
    "    s2=s2.tolist()\n",
    "    ss=ss+s2\n",
    "    ss.append(all_together(c)/10)\n",
    "    ss=np.array(ss)\n",
    "    return ss\n",
    "shapef_and_SOE_extent(lymph[-4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shape Features+ SOE+Extent+LBP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shapef_and_SOE_extent_LBP(c):\n",
    "\n",
    "    ss=SOE(c)\n",
    "    ss=ss.tolist()\n",
    "    s2=shape_features(c)\n",
    "    s2=s2.tolist()\n",
    "    s3=obj.describe(mask_cell(c))\n",
    "    s3=s3.tolist()\n",
    "    ss=ss+s2+s3\n",
    "    ss.append(all_together(c)/10)\n",
    "    ss=np.array(ss)\n",
    "    return ss\n",
    "shapef_and_SOE_extent_LBP(lymph[-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shapef_extent(c):\n",
    "\n",
    "    ss=shape_features(c)\n",
    "    ss=ss.tolist()\n",
    "    ss.append(all_together(c)/10)\n",
    "    ss=np.array(ss)\n",
    "    return ss\n",
    "shapef_extent(lymph[-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import time\n",
    "\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "import cv2\n",
    "\n",
    "# Load model into KerasLayer\n",
    "#model_url = \"https://tfhub.dev/google/bit/m-r50x1/1\"\n",
    "#model_url = \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_s/feature_vector/2\" #.99\n",
    "#model_url = \"https://tfhub.dev/google/imagenet/resnet_v2_152/feature_vector/5\"          #97\n",
    "model_url = \".\\imagenet_efficientnet_v2_imagenet21k_s_feature_vector_2\"\n",
    "module = hub.KerasLayer(model_url)\n",
    "\n",
    "\"\"\"\n",
    "module = tf.keras.Sequential([\n",
    "    hub.KerasLayer(\"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_s/feature_vector/2\",\n",
    "                   trainable=False),  # Can be True, see below.\n",
    "    #tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "module.build([None, 384, 384, 3])  # Batch input shape.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigTran(image):\n",
    "    image = np.array(image)\n",
    "    if len(image.shape)==2:\n",
    "        backtorgb = cv2.cvtColor(np.array(image, dtype=np.uint8),cv2.COLOR_GRAY2RGB)\n",
    "        image=backtorgb\n",
    "        \"\"\"\n",
    "        image2=np.zeros((image.shape[0], image.shape[1],3),np.int32)\n",
    "        image2[:,:,0]=image[:,:]\n",
    "        image2[:,:,1]=image[:,:]\n",
    "        image2[:,:,2]=image[:,:]\n",
    "        image=image2\n",
    "        \"\"\"\n",
    "    image=cv2.resize(image,(384,384))\n",
    "    # reshape into shape [batch_size, height, width, num_channels]\n",
    "    img_reshaped = tf.reshape(image, [1, image.shape[0], image.shape[1], image.shape[2]])\n",
    "    # Use `convert_image_dtype` to convert to floats in the [0,1] range.\n",
    "    image = tf.image.convert_image_dtype(img_reshaped, tf.float32)\n",
    "    logits=module(image)\n",
    "    return np.array(logits)[0]\n",
    " \n",
    "image=lymph[10]\n",
    "logits =bigTran(image) # module(preprocess_image(image))\n",
    "logits.shape\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.engine import  Model\n",
    "from keras.layers import Input\n",
    "from keras_vggface.vggface import VGGFace\n",
    "\n",
    "vgg_features = VGGFace(model='senet50',include_top=False, input_shape=(224, 224, 3), pooling='avg') # pooling: None, avg or max\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vggface3(image):\n",
    "    image = cv2.resize(image, (224, 224))\n",
    "    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "    logits=vgg_features.predict(image)\n",
    "    return logits[0]\n",
    "\n",
    "image=lymph[10]\n",
    "logits =vggface3(image) # module(preprocess_image(image))\n",
    "logits.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shapef_GLCM(c):\n",
    "\n",
    "    ss=shape_features(c)\n",
    "    ss=ss.tolist()\n",
    "    s2=GLCM(mask_cell(c))\n",
    "    s2=s2.tolist()\n",
    "    ss=ss+s2\n",
    "    ss=np.array(ss)\n",
    "    return ss\n",
    "shapef_GLCM(lymph[-4])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shapef_GLCM_SOE(c):\n",
    "\n",
    "    ss=shape_features(c)\n",
    "    ss=ss.tolist()\n",
    "    s2=GLCM(mask_cell(c))\n",
    "    s2=s2.tolist()\n",
    "    s3=SOE(c)\n",
    "    s3=s3.tolist()\n",
    "    ss=ss+s2+s3\n",
    "    ss=np.array(ss)\n",
    "    return ss\n",
    "shapef_GLCM_SOE(lymph[-4])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shapef_GLCM_SOE_HOG(c):\n",
    "\n",
    "    ss=shape_features(c)\n",
    "    ss=ss.tolist()\n",
    "    s2=GLCM(mask_cell(c))\n",
    "    s2=s2.tolist()\n",
    "    s3=SOE(c)\n",
    "    s3=s3.tolist()\n",
    "    s4=HOG_im(c)[0]\n",
    "    s4=s4.tolist()\n",
    "    ss=ss+s2+s3+s4\n",
    "    ss=np.array(ss)\n",
    "    return ss\n",
    "shapef_GLCM_SOE_HOG(lymph[-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj=LocalBinaryPatterns(points,8)\n",
    "\n",
    "def features(c, f):\n",
    "    if f==1:\n",
    "        return all_together(c) \n",
    "    elif f==2:\n",
    "        return obj.describe(mask_cell(c))\n",
    "    elif f==3:\n",
    "        return LBP_and_extent(c)\n",
    "    elif f==4:\n",
    "        return vggface3(c)\n",
    "    elif f==5:\n",
    "        return SOE_and_extent(c)\n",
    "    elif f==6:\n",
    "        return SOE_LBP_and_extent(c)\n",
    "    elif f==7:\n",
    "        return bigTran(mask_cell(c))\n",
    "    elif f==8:\n",
    "        return SOE(c)\n",
    "    elif f==9:\n",
    "        return shape_features(c)\n",
    "    elif f==10:\n",
    "        return shapef_and_SOE(c)    \n",
    "    elif f==11:\n",
    "        return shapef_and_SOE_extent(c)    \n",
    "    elif f==12:\n",
    "        return shapef_and_SOE_extent_LBP(c)  \n",
    "    elif f==13:\n",
    "        return shapef_extent(c)  \n",
    "    elif f==14:\n",
    "        return GLCM(mask_cell(c))\n",
    "    elif f==15:\n",
    "        return shapef_GLCM(c)\n",
    "    elif f==16:\n",
    "        return shapef_GLCM_SOE(c)  \n",
    "    elif f==17:\n",
    "        return HOG_im(c)[0]    \n",
    "    elif f==18:\n",
    "        return shapef_GLCM_SOE_HOG(c)\n",
    "    elif f==19:\n",
    "        #PCA\n",
    "        c=cv2.resize(mask_cell(c),(100,100))\n",
    "        return c.reshape(-1)  \n",
    "    \n",
    "    elif f==20:\n",
    "        #lda\n",
    "        c=cv2.resize(mask_cell(c),(100,100))\n",
    "        return c.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def alldata(cls,ft):\n",
    "\n",
    "    if cls==2:\n",
    "        lymph_extent = [ features(c, ft) for c in lymph]\n",
    "        neut_extent = [features(c, ft) for c in neut]\n",
    "        x = np.array(lymph_extent + neut_extent)#\n",
    "        if ft==1: x=x.reshape(-1, 1)\n",
    "        y = len(lymph_extent)*[0] + len(neut_extent)*[1]\n",
    "\n",
    "    \n",
    "    elif cls==3:\n",
    "        neut_extent = [features(c, ft) for c in neut]\n",
    "        lymph_extent = [features(c, ft) for c in lymph]\n",
    "        MONO_extent = [ features(c, ft)  for c  in MONO]\n",
    "        x = np.array(lymph_extent + neut_extent+ MONO_extent)#.reshape(-1, 1)\n",
    "        if ft==1: x=x.reshape(-1, 1)\n",
    "        y = len(lymph_extent)*[0] + len(neut_extent)*[1]+len(MONO_extent)*[2]\n",
    "        \n",
    "    elif cls==5:\n",
    "        neut_extent = [features(c, ft)  for c  in neut]\n",
    "        lymph_extent = [features(c, ft)  for c  in lymph]\n",
    "        BASO_extent = [ features(c, ft)  for c  in BASO]\n",
    "        EOSI_extent = [ features(c, ft)  for c in EOSI]\n",
    "        MONO_extent = [ features(c, ft)  for c  in MONO]\n",
    "        x = np.array(lymph_extent + neut_extent+ BASO_extent+ EOSI_extent+ MONO_extent)#.reshape(-1, 1)\n",
    "        if ft==1: x=x.reshape(-1, 1)\n",
    "        y = len(lymph_extent)*[0] + len(neut_extent)*[1]+len(BASO_extent)*[2]+len(EOSI_extent)*[3]+len(MONO_extent)*[4]\n",
    "        \n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def classif(cls,ft):\n",
    "    print('#########################################################################################')\n",
    "    print()\n",
    "    print(\"classes=\",cls,'   Features type=',ft)\n",
    "    print()\n",
    "    \n",
    "    if ft==19:  pca = PCA(n_components=50)\n",
    "    if ft==20:  lda = LinearDiscriminantAnalysis(n_components=4)\n",
    "        \n",
    "    x,y=alldata(cls,ft)\n",
    "    \n",
    "    print('training length=',len(x))\n",
    "    print()\n",
    "    print('#########################################################################################')    \n",
    "    names = [\"Decision Tree\",\"Nearest Neighbors\", \"Neural Net\", \"AdaBoost\", \"SVM\", \"GaussianNB\",\n",
    "             \"Random Forest\"]\n",
    "\n",
    "    classifiers = [tree.DecisionTreeClassifier(class_weight = 'balanced', max_depth = 1),\n",
    "                   KNeighborsClassifier(7),\n",
    "                   MLPClassifier(alpha=.1, max_iter=10000),\n",
    "                   AdaBoostClassifier(),\n",
    "                   SVC(gamma=2, C=1),\n",
    "                   GaussianNB(),\n",
    "                   RandomForestClassifier(max_depth=1, n_estimators=10, max_features=1)\n",
    "                   ]\n",
    "    classifiers=[MLPClassifier(alpha=.1, max_iter=10000)]\n",
    "    rkf=RepeatedKFold(n_splits=10, n_repeats=1, random_state=0)\n",
    "    y1=np.array(y)\n",
    "    for name, clf in zip(names, classifiers):\n",
    "        list=[]\n",
    "        list_tr=[]\n",
    "        for train, test in rkf.split(x):\n",
    "            #print(\"%s %s\" % (train, test),\"\\n\")\n",
    "            #print(y1[train])\n",
    "            X_train, X_test, y_train, y_test=x[train], x[test], y1[train], y1[test]\n",
    "            \n",
    "            if ft==19: #PCA\n",
    "                #X_train = StandardScaler().fit_transform(X_train)\n",
    "                #X_test = StandardScaler().fit_transform(X_test)\n",
    "                X_train=pca.fit_transform(X_train)\n",
    "                X_test= pca.transform(X_test)\n",
    "                \n",
    "            if ft==20: #LDA\n",
    "                #X_train = StandardScaler().fit_transform(X_train)\n",
    "                #X_test = StandardScaler().fit_transform(X_test)\n",
    "                X_train=lda.fit_transform(X_train,y_train)\n",
    "                X_test= lda.transform(X_test)\n",
    "                \n",
    "            clf.fit(X_train, y_train)\n",
    "            predictions=clf.predict(X_test)\n",
    "            #list.append(accuracy_score(y_test, predictions))\n",
    "            list.append(f1_score(y_test, predictions,average='micro'))\n",
    "            \n",
    "            list_tr.append(accuracy_score(y_train, clf.predict(X_train)))\n",
    "\n",
    "        print()\n",
    "        print(name)\n",
    "        #print(list_tr,\"\\n\")\n",
    "        print(\"Training Accuracy\",end=\" : \")\n",
    "        print(\"\\t\",np.array(list_tr).mean(),\"\\t\\terror +/-=\",np.array(list_tr).std())\n",
    "\n",
    "        #print(list,\"\\n\")\n",
    "        print(\"Test Accuracy\",end=\" : \")\n",
    "        print(\"\\t\",np.array(list).mean(),\"\\t\\terror +/-=\",np.array(list).std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for cls in [2,3,5]: \n",
    "    for ft in [9]:#[1,2,3]:\n",
    "        classif(cls,ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "classifiers = [tree.DecisionTreeClassifier(class_weight = 'balanced', max_depth = 1),  #0\n",
    "               KNeighborsClassifier(7),                                                #1\n",
    "               MLPClassifier(alpha=.1, max_iter=10000), #2\n",
    "               AdaBoostClassifier(),#3\n",
    "               SVC(gamma=2, C=1),#4\n",
    "               GaussianNB(),#5\n",
    "               RandomForestClassifier(max_depth=1, n_estimators=10, max_features=1)#6\n",
    "               ]\n",
    "\n",
    "cl=5\n",
    "fe=7\n",
    "clf= classifiers[2]\n",
    "\n",
    "x,y=alldata(cl,fe)\n",
    "X_train=x\n",
    "y_train=y\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(x, y, test_size = 0.1 )\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train, Y_train)\n",
    "\n",
    "\n",
    "predictions=clf.predict(X_val)\n",
    "print(accuracy_score(Y_val, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(Y_val, predictions , target_names=[\"lymph\",\"neut\",\"BASO\",\"EOSI\",\"MONO\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(Y_val, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "lymph = [cv2.imread(f)[:,:,::-1] for f in glob.glob('./samples/PBC/LYMPH/*.jpg')]\n",
    "neut =  [cv2.imread(f)[:,:,::-1] for f in glob.glob('./samples/PBC/NEUT/*.jpg')]\n",
    "BASO =  [cv2.imread(f)[:,:,::-1] for f in glob.glob('./samples/PBC/BASO/*.jpg')]\n",
    "EOSI =  [cv2.imread(f)[:,:,::-1] for f in glob.glob('./samples/PBC/EOSI/*.jpg')]\n",
    "MONO =  [cv2.imread(f)[:,:,::-1] for f in glob.glob('./samples/PBC/MONO/*.jpg')]\n",
    "\n",
    "\n",
    "classifiers = [tree.DecisionTreeClassifier(class_weight = 'balanced', max_depth = 1),  #0\n",
    "               KNeighborsClassifier(7),                                                #1\n",
    "               MLPClassifier(alpha=.1, max_iter=10000), #2\n",
    "               AdaBoostClassifier(),#3\n",
    "               SVC(gamma=2, C=1),#4\n",
    "               GaussianNB(),#5\n",
    "               RandomForestClassifier(max_depth=1, n_estimators=10, max_features=1)#6\n",
    "               ]\n",
    "\n",
    "cl=2\n",
    "fe=15\n",
    "clf= classifiers[2]\n",
    "\n",
    "x,y=alldata(cl,fe)\n",
    "X_train=x\n",
    "y_train=y\n",
    "clf.fit(X_train, y_train)\n",
    "pickle.dump(clf, open('test.pkl', 'wb'))\n",
    "\n",
    "predictions=clf.predict(X_train)\n",
    "print(accuracy_score(y_train, predictions))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BCCD Dataset The labels (1- 5) represent neutrophil, lymphocyte, monocyte, eosinophil and basophil, respectively.\n",
    "\n",
    "data1 = [cv2.imread(f)[:,:,::-1] for f in glob.glob('./samples/BCCD/Dataset1/*.bmp')]\n",
    "data2 = [cv2.imread(f)[:,:,::-1] for f in glob.glob('./samples/BCCD/Dataset2/*.bmp')]\n",
    "\n",
    "\n",
    "labels1 = pd.read_csv(\"./samples/BCCD/Class Labels of Dataset 1.csv\") \n",
    "lst1=labels1['class label'].tolist()\n",
    "\n",
    "labels2 = pd.read_csv(\"./samples/BCCD/Class Labels of Dataset 2.csv\") \n",
    "lst2=labels2['class'].tolist()\n",
    "\n",
    "ims=data1#data2+data1\n",
    "lst=lst1# lst2+lst1\n",
    "\n",
    "neut = [ims[i] for i,c in enumerate(lst) if c==1] #+neut\n",
    "lymph = [ims[i] for i,c in enumerate(lst) if c==2] #+lymph\n",
    "MONO =  [ims[i] for i,c in enumerate(lst) if c==3] #+MONO\n",
    "EOSI = [ims[i] for i,c in enumerate(lst) if c==4] #+EOSI \n",
    "BASO =  [ims[i] for i,c in enumerate(lst) if c==5] #+BASO\n",
    "\n",
    "\n",
    "x,y=alldata(cl,fe)\n",
    "\n",
    "\n",
    "pickled_model = pickle.load(open('test.pkl', 'rb'))\n",
    "predictions=pickled_model.predict(x)\n",
    "accuracy_score(y, predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mic_img = [cv2.imread(f)[:,:,::-1] for f in glob.glob('./samples/QSb MHS/QSb MHS/*.jpg')]; th=80\n",
    "mic_img = [cv2.imread(f)[:,:,::-1] for f in glob.glob('./samples/Q2_#2/Q2_#2/*.png')]; th=30\n",
    "\n",
    "plt.imshow(mic_img[2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img=mic_img[0].copy()\n",
    "cv2.imwrite(\"wbc.png\",img)\n",
    "r=img.shape[0]\n",
    "c=img.shape[1]\n",
    "cn=0\n",
    "ls_img=[]\n",
    "for i in range(21,r-20):\n",
    "    for j in range(21,c-20):\n",
    "        if (int(img[i,j,2])-int(img[i,j,0]))>th: \n",
    "            im=img[i-20:i+20,j-20:j+20,:].copy()\n",
    "            img[i-20:i+20,j-20:j+20,:]=0\n",
    "            ls_img.append(im)\n",
    "            cn=cn+1\n",
    "            #plt.imshow(im)\n",
    "            #plt.show()\n",
    "cn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    " \n",
    "from matplotlib.pyplot import figure\n",
    "cell_type=['LYMPH','NEUT','BASO','EOSI','MONO']\n",
    "cell_type=['LYMPH','NEUT','MONO']\n",
    "\n",
    " \n",
    "for im in ls_img :\n",
    "    print( cell_type[pickled_model.predict([features(im, fe)])[0]])\n",
    "    \n",
    "    figure(figsize=(7, 7))\n",
    "\n",
    "    plt.subplot(121)\n",
    "    plt.imshow(im)\n",
    "    plt.subplot(122)\n",
    "    plt.imshow(deep_segment(im))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
